{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Import Libraries",
   "id": "f765bc1939c0c031"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-12-25T03:14:04.228994Z",
     "start_time": "2025-12-25T03:14:04.225665Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "from datetime import time\n",
    "import numpy as np"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-25T03:14:04.349654Z",
     "start_time": "2025-12-25T03:14:04.347641Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ---------------------------------------------------------\n",
    "# 1. Configuration: Project definitions\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "DATA_DIR = '../../data'\n",
    "\n",
    "# List of quarters designated as \"In-Sample\" for strategy selection\n",
    "IN_SAMPLE_QUARTERS = [\n",
    "    \"2023_Q1\", \"2023_Q3\", \"2023_Q4\",\n",
    "    \"2024_Q2\", \"2024_Q4\",\n",
    "    \"2025_Q1\", \"2025_Q2\"\n",
    "]"
   ],
   "id": "bed9cb7f586fe93c",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-25T03:14:04.362086Z",
     "start_time": "2025-12-25T03:14:04.357452Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ---------------------------------------------------------\n",
    "# 2. Data Loading Function\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "def load_project_data(data_dir):\n",
    "    \"\"\"\n",
    "    Loads Parquet files, sets 'datetime' as index, and removes timezone info (+00:00).\n",
    "    \"\"\"\n",
    "    file_paths = glob.glob(os.path.join(data_dir, \"*.parquet\"))\n",
    "\n",
    "    data_store = {\n",
    "        \"group1\": {},\n",
    "        \"group2\": {}\n",
    "    }\n",
    "\n",
    "    print(f\"Found {len(file_paths)} files. Loading and processing...\")\n",
    "\n",
    "    for path in file_paths:\n",
    "        filename = os.path.basename(path)\n",
    "        match = re.match(r\"(data[12])_(\\d{4})_(Q\\d)\\.parquet\", filename)\n",
    "\n",
    "        if match:\n",
    "            raw_group = match.group(1)\n",
    "            group_id = \"group\" + raw_group[-1]\n",
    "            year = match.group(2)\n",
    "            quarter = match.group(3)\n",
    "            quarter_id = f\"{year}_{quarter}\"\n",
    "\n",
    "            try:\n",
    "                # 1. Load Data\n",
    "                df = pd.read_parquet(path)\n",
    "\n",
    "                # 2. Set 'datetime' column as Index\n",
    "                if 'datetime' in df.columns:\n",
    "                    df = df.set_index('datetime')\n",
    "\n",
    "                # 3. Ensure Index is Datetime Object\n",
    "                df.index = pd.to_datetime(df.index)\n",
    "\n",
    "                # 4. Remove Timezone Information (+00:00)\n",
    "                # tz_localize(None) strips the timezone, keeping the local time\n",
    "                if df.index.tz is not None:\n",
    "                    df.index = df.index.tz_localize(None)\n",
    "\n",
    "                # 5. Sort by Time\n",
    "                df = df.sort_index()\n",
    "\n",
    "                # 6. Add Metadata\n",
    "                df['Quarter_ID'] = quarter_id\n",
    "                df['Is_In_Sample'] = quarter_id in IN_SAMPLE_QUARTERS\n",
    "\n",
    "                # Store\n",
    "                data_store[group_id][quarter_id] = df\n",
    "\n",
    "                status = \"In-Sample\" if df['Is_In_Sample'].iloc[0] else \"Out-of-Sample\"\n",
    "                print(f\"Loaded {filename} -> {group_id} [{status}] | Timezone Removed.\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {filename}: {e}\")\n",
    "\n",
    "    return data_store"
   ],
   "id": "5a174da3e8cdc694",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-25T03:14:04.375032Z",
     "start_time": "2025-12-25T03:14:04.372649Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ---------------------------------------------------------\n",
    "# 3. Helper Function to Combine Data\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "def combine_quarters(data_store, group_id, only_in_sample=True):\n",
    "    if group_id not in data_store:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    quarters_dict = data_store[group_id]\n",
    "    df_list = []\n",
    "\n",
    "    for q_id, df in quarters_dict.items():\n",
    "        if only_in_sample and not df['Is_In_Sample'].iloc[0]:\n",
    "            continue\n",
    "        df_list.append(df)\n",
    "\n",
    "    if df_list:\n",
    "        combined_df = pd.concat(df_list)\n",
    "        combined_df = combined_df.sort_index()\n",
    "        return combined_df\n",
    "    else:\n",
    "        return pd.DataFrame()"
   ],
   "id": "43acc6808ca87e88",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-25T03:14:04.551660Z",
     "start_time": "2025-12-25T03:14:04.382080Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ---------------------------------------------------------\n",
    "# 4. Execution Example\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    raw_data = load_project_data(DATA_DIR)\n",
    "\n",
    "    # Create combined training set\n",
    "    df_g1_train = combine_quarters(raw_data, \"group1\", only_in_sample=True)\n",
    "\n",
    "    print(\"-\" * 40)\n",
    "    if not df_g1_train.empty:\n",
    "        print(\"Verifying Index (Should be simple Datetime without +00:00):\")\n",
    "        print(df_g1_train.index)\n",
    "\n",
    "        print(\"\\nHead Sample:\")\n",
    "        print(df_g1_train.head(5))\n",
    "        print(df_g1_train.tail(5))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 1. Load all data (if not already loaded)\n",
    "    # raw_data = load_project_data(DATA_DIR)\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # Create Group 2 Data (In-Sample)\n",
    "    # ---------------------------------------------------------\n",
    "    print(\"Processing Group 2 (CAD, AUD, XAU, XAG)...\")\n",
    "\n",
    "    df_g2_train = combine_quarters(raw_data, \"group2\", only_in_sample=True)\n",
    "\n",
    "    print(\"-\" * 40)\n",
    "    if not df_g2_train.empty:\n",
    "        print(\"Group 2 Shape:\")\n",
    "        print(df_g2_train.shape)\n",
    "\n",
    "        print(\"\\nGroup 2 Columns (Should be CAD, AUD, XAU, XAG, etc.):\")\n",
    "        print(df_g2_train.columns)\n",
    "\n",
    "        print(\"\\nGroup 2 Head Sample (5-min freq, No Timezone):\")\n",
    "        print(df_g2_train.head(3))\n",
    "\n",
    "        print(\"\\nVerifying Index Type:\")\n",
    "        print(df_g2_train.index.dtype)\n",
    "    else:\n",
    "        print(\"Group 2 data is empty. Please check the file names (data2_...).\")"
   ],
   "id": "d0cc6c44d740b4b6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 14 files. Loading and processing...\n",
      "Loaded data2_2023_Q4.parquet -> group2 [In-Sample] | Timezone Removed.\n",
      "Loaded data1_2023_Q3.parquet -> group1 [In-Sample] | Timezone Removed.\n",
      "Loaded data2_2025_Q1.parquet -> group2 [In-Sample] | Timezone Removed.\n",
      "Loaded data1_2023_Q1.parquet -> group1 [In-Sample] | Timezone Removed.\n",
      "Loaded data2_2025_Q2.parquet -> group2 [In-Sample] | Timezone Removed.\n",
      "Loaded data2_2024_Q4.parquet -> group2 [In-Sample] | Timezone Removed.\n",
      "Loaded data1_2024_Q2.parquet -> group1 [In-Sample] | Timezone Removed.\n",
      "Loaded data2_2023_Q3.parquet -> group2 [In-Sample] | Timezone Removed.\n",
      "Loaded data1_2025_Q1.parquet -> group1 [In-Sample] | Timezone Removed.\n",
      "Loaded data1_2023_Q4.parquet -> group1 [In-Sample] | Timezone Removed.\n",
      "Loaded data1_2024_Q4.parquet -> group1 [In-Sample] | Timezone Removed.\n",
      "Loaded data2_2024_Q2.parquet -> group2 [In-Sample] | Timezone Removed.\n",
      "Loaded data1_2025_Q2.parquet -> group1 [In-Sample] | Timezone Removed.\n",
      "Loaded data2_2023_Q1.parquet -> group2 [In-Sample] | Timezone Removed.\n",
      "----------------------------------------\n",
      "Verifying Index (Should be simple Datetime without +00:00):\n",
      "DatetimeIndex(['2023-01-02 09:31:00', '2023-01-02 09:32:00',\n",
      "               '2023-01-02 09:33:00', '2023-01-02 09:34:00',\n",
      "               '2023-01-02 09:35:00', '2023-01-02 09:36:00',\n",
      "               '2023-01-02 09:37:00', '2023-01-02 09:38:00',\n",
      "               '2023-01-02 09:39:00', '2023-01-02 09:40:00',\n",
      "               ...\n",
      "               '2025-06-30 15:51:00', '2025-06-30 15:52:00',\n",
      "               '2025-06-30 15:53:00', '2025-06-30 15:54:00',\n",
      "               '2025-06-30 15:55:00', '2025-06-30 15:56:00',\n",
      "               '2025-06-30 15:57:00', '2025-06-30 15:58:00',\n",
      "               '2025-06-30 15:59:00', '2025-06-30 16:00:00'],\n",
      "              dtype='datetime64[us]', name='datetime', length=175812, freq=None)\n",
      "\n",
      "Head Sample:\n",
      "                     NQ  SP Quarter_ID  Is_In_Sample\n",
      "datetime                                            \n",
      "2023-01-02 09:31:00 NaN NaN    2023_Q1          True\n",
      "2023-01-02 09:32:00 NaN NaN    2023_Q1          True\n",
      "2023-01-02 09:33:00 NaN NaN    2023_Q1          True\n",
      "2023-01-02 09:34:00 NaN NaN    2023_Q1          True\n",
      "2023-01-02 09:35:00 NaN NaN    2023_Q1          True\n",
      "                            NQ        SP Quarter_ID  Is_In_Sample\n",
      "datetime                                                         \n",
      "2025-06-30 15:56:00  22699.298  6208.279    2025_Q2          True\n",
      "2025-06-30 15:57:00  22689.619  6206.774    2025_Q2          True\n",
      "2025-06-30 15:58:00  22706.510  6211.059    2025_Q2          True\n",
      "2025-06-30 15:59:00  22697.721  6208.543    2025_Q2          True\n",
      "2025-06-30 16:00:00  22669.665  6200.242    2025_Q2          True\n",
      "Processing Group 2 (CAD, AUD, XAU, XAG)...\n",
      "----------------------------------------\n",
      "Group 2 Shape:\n",
      "(129770, 6)\n",
      "\n",
      "Group 2 Columns (Should be CAD, AUD, XAU, XAG, etc.):\n",
      "Index(['AUD', 'CAD', 'XAG', 'XAU', 'Quarter_ID', 'Is_In_Sample'], dtype='object')\n",
      "\n",
      "Group 2 Head Sample (5-min freq, No Timezone):\n",
      "                         AUD  CAD  XAG  XAU Quarter_ID  Is_In_Sample\n",
      "datetime                                                            \n",
      "2023-01-01 17:35:00      NaN  NaN  NaN  NaN    2023_Q1          True\n",
      "2023-01-01 18:00:00      NaN  NaN  NaN  NaN    2023_Q1          True\n",
      "2023-01-01 18:05:00  0.68142  NaN  NaN  NaN    2023_Q1          True\n",
      "\n",
      "Verifying Index Type:\n",
      "datetime64[us]\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-25T03:14:04.582180Z",
     "start_time": "2025-12-25T03:14:04.567958Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df_g1_train.info()\n",
    "df_g2_train.info()"
   ],
   "id": "c792fcbdd5196fc8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 175812 entries, 2023-01-02 09:31:00 to 2025-06-30 16:00:00\n",
      "Data columns (total 4 columns):\n",
      " #   Column        Non-Null Count   Dtype  \n",
      "---  ------        --------------   -----  \n",
      " 0   NQ            175423 non-null  float64\n",
      " 1   SP            175423 non-null  float64\n",
      " 2   Quarter_ID    175812 non-null  object \n",
      " 3   Is_In_Sample  175812 non-null  bool   \n",
      "dtypes: bool(1), float64(2), object(1)\n",
      "memory usage: 5.5+ MB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 129770 entries, 2023-01-01 17:35:00 to 2025-06-30 23:55:00\n",
      "Data columns (total 6 columns):\n",
      " #   Column        Non-Null Count   Dtype  \n",
      "---  ------        --------------   -----  \n",
      " 0   AUD           124578 non-null  float64\n",
      " 1   CAD           124577 non-null  float64\n",
      " 2   XAG           124320 non-null  float64\n",
      " 3   XAU           124320 non-null  float64\n",
      " 4   Quarter_ID    129770 non-null  object \n",
      " 5   Is_In_Sample  129770 non-null  bool   \n",
      "dtypes: bool(1), float64(4), object(1)\n",
      "memory usage: 6.1+ MB\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-25T03:14:04.599986Z",
     "start_time": "2025-12-25T03:14:04.597103Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ---------------------------------------------------------\n",
    "# Configuration: Time Rules\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "# Group 1 (1-min) Rules\n",
    "G1_CALC_EXCLUDE_START = time(9, 31)\n",
    "G1_CALC_EXCLUDE_END   = time(9, 40)  # First 10 mins\n",
    "G1_TRADE_START        = time(9, 56)  # Start trading AFTER 9:55\n",
    "G1_EXIT_TIME          = time(15, 40) # Exit 20 mins before session end\n",
    "G1_SESSION_END_START  = time(15, 51)\n",
    "G1_SESSION_END_FINISH = time(16, 00) # Last 10 mins\n",
    "\n",
    "# Group 2 (5-min) Rules\n",
    "G2_EXIT_TIME          = time(16, 50) # 10 mins before break\n",
    "G2_BREAK_START        = time(17, 00)\n",
    "G2_BREAK_END          = time(18, 00)\n",
    "G2_TRADE_RESTART      = time(18, 10) # 10 mins after break"
   ],
   "id": "5ec889a16d27a62f",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-25T03:14:04.613230Z",
     "start_time": "2025-12-25T03:14:04.611063Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ---------------------------------------------------------\n",
    "# Processing Function\n",
    "# ---------------------------------------------------------\n",
    "def apply_common_assumptions(df, group_type):\n",
    "    if df.empty: return df\n",
    "\n",
    "    # Defaults\n",
    "    df['can_trade'] = False\n",
    "    df['force_exit'] = False\n",
    "    t = df.index.time\n",
    "\n",
    "    if group_type == \"group2\":\n",
    "        # === CRITICAL FIX: ROW REMOVAL ===\n",
    "        # Remove 16:50 <= t < 18:10\n",
    "        # Keep: t < 16:50 OR t >= 18:10\n",
    "        keep_mask = (t < time(16, 50)) | (t >= time(18, 10))\n",
    "        df = df.loc[keep_mask].copy()\n",
    "\n",
    "        # Re-index time after removal\n",
    "        t = df.index.time\n",
    "\n",
    "        # Set flags\n",
    "        df['can_trade'] = True\n",
    "\n",
    "        # Force exit at the last bar before removal (16:45)\n",
    "        df.loc[t == time(16, 45), 'force_exit'] = True\n",
    "\n",
    "    return df"
   ],
   "id": "60a4d4adaa49756a",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-25T03:14:04.740741Z",
     "start_time": "2025-12-25T03:14:04.623863Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ==========================================\n",
    "# 2. Execution & Verification (Run this part!)\n",
    "# ==========================================\n",
    "\n",
    "# 1. Reload original data (to be sure)\n",
    "# df_g2_train = pd.read_pickle('df_g2_processed.pkl') # If loading from file\n",
    "# OR use your existing variable if it's the raw one\n",
    "\n",
    "print(f\"Rows BEFORE cleanup: {len(df_g2_train)}\")\n",
    "\n",
    "# 2. APPLY THE FIX and ASSIGN to a new variable\n",
    "df_g2_clean = apply_common_assumptions(df_g2_train.copy(), \"group2\")\n",
    "\n",
    "print(f\"Rows AFTER cleanup:  {len(df_g2_clean)}\")\n",
    "\n",
    "# 3. VERIFY\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"FINAL CHECK\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# Check A: Bar count (Target: 272)\n",
    "daily_counts = df_g2_clean.groupby(df_g2_clean.index.date).size()\n",
    "mode_count = daily_counts.mode()[0]\n",
    "print(f\"Bars per day (Mode): {mode_count}  <-- Should be 272\")\n",
    "\n",
    "# Check B: Gap Check\n",
    "# Is there any data at 16:50 or 18:05?\n",
    "gap_check = df_g2_clean[\n",
    "    (df_g2_clean.index.time == time(16, 50)) |\n",
    "    (df_g2_clean.index.time == time(18, 5))\n",
    "]\n",
    "\n",
    "if gap_check.empty:\n",
    "    print(\"Gap Check: ✅ OK (Gap rows removed)\")\n",
    "else:\n",
    "    print(\"Gap Check: ❌ FAILED (Rows still exist!)\")\n",
    "    print(gap_check.head())\n",
    "\n",
    "# Check C: Start Time\n",
    "# The first bar after the gap should be 18:10\n",
    "last_session_start = df_g2_clean.between_time('18:00', '18:20').index.time\n",
    "if len(last_session_start) > 0:\n",
    "    print(f\"Sample time around restart: {last_session_start[0]} <-- Should be 18:10\")"
   ],
   "id": "dc2f011d7589f723",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows BEFORE cleanup: 129770\n",
      "Rows AFTER cleanup:  122791\n",
      "\n",
      "========================================\n",
      "FINAL CHECK\n",
      "========================================\n",
      "Bars per day (Mode): 272  <-- Should be 272\n",
      "Gap Check: ✅ OK (Gap rows removed)\n",
      "Sample time around restart: 18:15:00 <-- Should be 18:10\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-25T03:14:04.986857Z",
     "start_time": "2025-12-25T03:14:04.755114Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ---------------------------------------------------------\n",
    "# Execution\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Assume df_g1_train and df_g2_train are already loaded from previous step\n",
    "\n",
    "    print(\"Applying Common Assumptions to Group 1...\")\n",
    "    df_g1_clean = apply_common_assumptions(df_g1_train.copy(), \"group1\")\n",
    "\n",
    "    print(\"Applying Common Assumptions to Group 2...\")\n",
    "    df_g2_clean = apply_common_assumptions(df_g2_train.copy(), \"group2\")\n",
    "\n",
    "    # --- Verification ---\n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "    print(\"VERIFICATION GROUP 1 (SP/NQ)\")\n",
    "    print(\"=\"*40)\n",
    "\n",
    "    # Check Morning NaN Mask (09:31 - 09:40)\n",
    "    test_time_nan = time(9, 35)\n",
    "    row_nan = df_g1_clean[df_g1_clean.index.time == test_time_nan]\n",
    "    if not row_nan.empty:\n",
    "        print(f\"Time {test_time_nan} (Should be NaN):\")\n",
    "        print(row_nan[['SP', 'NQ']].head(1))\n",
    "\n",
    "    # Check Trading Start (09:56)\n",
    "    test_time_trade = time(9, 56)\n",
    "    row_trade = df_g1_clean[df_g1_clean.index.time == test_time_trade]\n",
    "    if not row_trade.empty:\n",
    "        print(f\"\\nTime {test_time_trade} (can_trade should be True):\")\n",
    "        print(row_trade[['can_trade']].head(1))\n",
    "\n",
    "    # Check Exit (15:40)\n",
    "    test_time_exit = time(15, 40)\n",
    "    row_exit = df_g1_clean[df_g1_clean.index.time == test_time_exit]\n",
    "    if not row_exit.empty:\n",
    "        print(f\"\\nTime {test_time_exit} (force_exit should be True):\")\n",
    "        print(row_exit[['force_exit', 'can_trade']].head(1))\n",
    "\n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "    print(\"VERIFICATION GROUP 2 (CAD/AUD...)\")\n",
    "    print(\"=\"*40)\n",
    "\n",
    "    # Check Post-Break Block (18:05 - Should be False)\n",
    "    test_g2_block = time(18, 5) # 5 min freq\n",
    "    row_g2_block = df_g2_clean[df_g2_clean.index.time == test_g2_block]\n",
    "    if not row_g2_block.empty:\n",
    "        print(f\"Time {test_g2_block} (can_trade should be False):\")\n",
    "        print(row_g2_block[['can_trade']].head(1))\n",
    "\n",
    "    # Check Restart (18:10 - Should be True)\n",
    "    test_g2_start = time(18, 10)\n",
    "    row_g2_start = df_g2_clean[df_g2_clean.index.time == test_g2_start]\n",
    "    if not row_g2_start.empty:\n",
    "        print(f\"Time {test_g2_start} (can_trade should be True):\")\n",
    "        print(row_g2_start[['can_trade']].head(1))"
   ],
   "id": "cb742f6444c9745e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying Common Assumptions to Group 1...\n",
      "Applying Common Assumptions to Group 2...\n",
      "\n",
      "========================================\n",
      "VERIFICATION GROUP 1 (SP/NQ)\n",
      "========================================\n",
      "Time 09:35:00 (Should be NaN):\n",
      "                     SP  NQ\n",
      "datetime                   \n",
      "2023-01-02 09:35:00 NaN NaN\n",
      "\n",
      "Time 09:56:00 (can_trade should be True):\n",
      "                     can_trade\n",
      "datetime                      \n",
      "2023-01-02 09:56:00      False\n",
      "\n",
      "Time 15:40:00 (force_exit should be True):\n",
      "                     force_exit  can_trade\n",
      "datetime                                  \n",
      "2023-01-02 15:40:00       False      False\n",
      "\n",
      "========================================\n",
      "VERIFICATION GROUP 2 (CAD/AUD...)\n",
      "========================================\n",
      "Time 18:10:00 (can_trade should be True):\n",
      "                     can_trade\n",
      "datetime                      \n",
      "2023-01-02 18:10:00       True\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-25T03:14:05.005038Z",
     "start_time": "2025-12-25T03:14:05.001932Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ==========================================\n",
    "# Function to Add Returns and Volatility\n",
    "# ==========================================\n",
    "def add_return_and_volatility(df, assets, interval_min, hours_per_day, vol_window=20):\n",
    "    \"\"\"\n",
    "    Adds Log Returns and Annualized Volatility columns to the DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - df: Input DataFrame\n",
    "    - assets: List of asset column names (e.g., ['SP', 'NQ'])\n",
    "    - interval_min: Data frequency in minutes (1 or 5)\n",
    "    - hours_per_day: Trading hours per day for annualization\n",
    "    - vol_window: Rolling window size for volatility calculation (default: 20 bars)\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # 1. Calculate Annualization Factor\n",
    "    # Calculate how many bars exist in one trading day\n",
    "    bars_per_day = (hours_per_day * 60) / interval_min\n",
    "\n",
    "    # Standard assumption: 252 trading days per year\n",
    "    # Annualization Factor = sqrt(Daily Bars * 252)\n",
    "    annual_factor = np.sqrt(bars_per_day * 252)\n",
    "\n",
    "    print(f\"Processing assets: {assets}\")\n",
    "    print(f\"  - Frequency: {interval_min} min\")\n",
    "    print(f\"  - Trading Hours/Day: {hours_per_day}\")\n",
    "    print(f\"  - Annualization Factor: {annual_factor:.2f}\")\n",
    "\n",
    "    # 2. Loop through assets to create new columns\n",
    "    for asset in assets:\n",
    "        if asset in df.columns:\n",
    "            # Log Return: ln(Current / Previous)\n",
    "            df[f'{asset}_rtn'] = np.log(df[asset] / df[asset].shift(1))\n",
    "\n",
    "            # Annualized Volatility: Rolling Std Dev * Annualization Factor\n",
    "            df[f'{asset}_vol'] = df[f'{asset}_rtn'].rolling(window=vol_window).std() * annual_factor\n",
    "        else:\n",
    "            print(f\"  [Warning] Asset '{asset}' not found in DataFrame.\")\n",
    "\n",
    "    return df"
   ],
   "id": "1576db920c3798e0",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-25T03:14:05.048406Z",
     "start_time": "2025-12-25T03:14:05.022316Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ==========================================\n",
    "# Execution\n",
    "# ==========================================\n",
    "\n",
    "# --- Group 1 Processing ---\n",
    "# Assets: SP, NQ\n",
    "# Frequency: 1 min\n",
    "# Trading Hours: 9:30 - 16:00 CET = 6.5 hours\n",
    "df_g1_train = add_return_and_volatility(\n",
    "    df_g1_train,\n",
    "    assets=['SP', 'NQ'],\n",
    "    interval_min=1,\n",
    "    hours_per_day=6.5\n",
    ")\n",
    "\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# --- Group 2 Processing ---\n",
    "# Assets: AUD, CAD, XAG, XAU\n",
    "# Frequency: 5 min\n",
    "# Trading Hours: Almost 24h with 1h break = 23 hours\n",
    "# (Note: Using list comprehension to ensure we only process columns that exist in the dataframe)\n",
    "g2_assets = ['AUD', 'CAD', 'XAG', 'XAU']\n",
    "existing_g2_assets = [col for col in g2_assets if col in df_g2_train.columns]\n",
    "\n",
    "df_g2_train = add_return_and_volatility(\n",
    "    df_g2_train,\n",
    "    assets=existing_g2_assets,\n",
    "    interval_min=5,\n",
    "    hours_per_day=23\n",
    ")"
   ],
   "id": "365988c4bbe3cc10",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing assets: ['SP', 'NQ']\n",
      "  - Frequency: 1 min\n",
      "  - Trading Hours/Day: 6.5\n",
      "  - Annualization Factor: 313.50\n",
      "------------------------------\n",
      "Processing assets: ['AUD', 'CAD', 'XAG', 'XAU']\n",
      "  - Frequency: 5 min\n",
      "  - Trading Hours/Day: 23\n",
      "  - Annualization Factor: 263.73\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-25T03:14:05.071671Z",
     "start_time": "2025-12-25T03:14:05.066016Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ==========================================\n",
    "# Verification\n",
    "# ==========================================\n",
    "print(\"\\n--- Group 1: SP Data Check (Last 5 rows) ---\")\n",
    "print(df_g1_train[['SP', 'SP_rtn', 'SP_vol']].tail())\n",
    "\n",
    "print(\"\\n--- Group 2: First Asset Data Check (Last 5 rows) ---\")\n",
    "if existing_g2_assets:\n",
    "    first_asset = existing_g2_assets[0]\n",
    "    print(df_g2_train[[first_asset, f'{first_asset}_rtn', f'{first_asset}_vol']].tail())"
   ],
   "id": "e465f31f320d1de3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Group 1: SP Data Check (Last 5 rows) ---\n",
      "                           SP    SP_rtn    SP_vol\n",
      "datetime                                         \n",
      "2025-06-30 15:56:00  6208.279 -0.000739  0.109098\n",
      "2025-06-30 15:57:00  6206.774 -0.000242  0.108644\n",
      "2025-06-30 15:58:00  6211.059  0.000690  0.112746\n",
      "2025-06-30 15:59:00  6208.543 -0.000405  0.118266\n",
      "2025-06-30 16:00:00  6200.242 -0.001338  0.154946\n",
      "\n",
      "--- Group 2: First Asset Data Check (Last 5 rows) ---\n",
      "                         AUD   AUD_rtn   AUD_vol\n",
      "datetime                                        \n",
      "2025-06-30 23:35:00  0.65672 -0.000152  0.068006\n",
      "2025-06-30 23:40:00  0.65691  0.000289  0.068987\n",
      "2025-06-30 23:45:00  0.65681 -0.000152  0.069758\n",
      "2025-06-30 23:50:00  0.65696  0.000228  0.070068\n",
      "2025-06-30 23:55:00  0.65679 -0.000259  0.072833\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "eabf29f0d8666c90"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-25T03:14:05.106359Z",
     "start_time": "2025-12-25T03:14:05.091169Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Save processed dataframes to pickle files for the next step (EDA)\n",
    "# This preserves column types and indices perfectly.\n",
    "df_g1_clean.to_pickle('df_g1_processed.pkl')\n",
    "df_g2_clean.to_pickle('df_g2_processed.pkl')\n",
    "\n",
    "print(\"Data saved successfully to .pkl files.\")"
   ],
   "id": "851dabc4e4a62b57",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved successfully to .pkl files.\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-25T03:14:05.132521Z",
     "start_time": "2025-12-25T03:14:05.131354Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "d3d54b85e94cdd51",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
