{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Import Libraries",
   "id": "f765bc1939c0c031"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2026-01-14T14:26:45.856987Z",
     "start_time": "2026-01-14T14:26:45.855140Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "from datetime import time, timedelta\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-14T14:26:45.968982Z",
     "start_time": "2026-01-14T14:26:45.967266Z"
    }
   },
   "cell_type": "code",
   "source": [
    "DATA_DIR = '../../data'\n",
    "\n",
    "# List of quarters designated as \"In-Sample\"\n",
    "IN_SAMPLE_QUARTERS = [\n",
    "    \"2023_Q1\", \"2023_Q3\", \"2023_Q4\",\n",
    "    \"2024_Q2\", \"2024_Q4\",\n",
    "    \"2025_Q1\", \"2025_Q2\"\n",
    "]\n",
    "\n",
    "# Time Zone Setting (CET/CEST)\n",
    "# \"Europe/Warsaw\" or \"Europe/Paris\" handles CET and DST (CEST) correctly.\n",
    "TARGET_TZ = 'Europe/Warsaw'"
   ],
   "id": "bed9cb7f586fe93c",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-14T14:26:45.996478Z",
     "start_time": "2026-01-14T14:26:45.990729Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ---------------------------------------------------------\n",
    "# 2. Data Loading Function\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "def load_project_data(data_dir, assume_utc=True):\n",
    "    \"\"\"\n",
    "    Loads Parquet files, manages timezone conversion strictly, and adds metadata.\n",
    "    \"\"\"\n",
    "    file_paths = glob.glob(os.path.join(data_dir, \"*.parquet\"))\n",
    "\n",
    "    if not file_paths:\n",
    "        print(f\"[Warning] No .parquet files found in {data_dir}\")\n",
    "        return {}\n",
    "\n",
    "    data_store = {\n",
    "        \"group1\": {},\n",
    "        \"group2\": {}\n",
    "    }\n",
    "\n",
    "    print(f\"Found {len(file_paths)} files. Loading...\")\n",
    "\n",
    "    for path in file_paths:\n",
    "        filename = os.path.basename(path)\n",
    "        match = re.match(r\"(data[12])_(\\d{4})_(Q\\d)\\.parquet\", filename)\n",
    "\n",
    "        if match:\n",
    "            raw_group = match.group(1)\n",
    "            group_id = \"group\" + raw_group[-1]\n",
    "            year = match.group(2)\n",
    "            quarter = match.group(3)\n",
    "            quarter_id = f\"{year}_{quarter}\"\n",
    "\n",
    "            try:\n",
    "                # 1. Load Data\n",
    "                df = pd.read_parquet(path)\n",
    "\n",
    "                # 2. Set 'datetime' column as Index\n",
    "                if 'datetime' in df.columns:\n",
    "                    df = df.set_index('datetime')\n",
    "\n",
    "                # 3. Ensure Index is Datetime Object\n",
    "                df.index = pd.to_datetime(df.index)\n",
    "\n",
    "                # 4. Timezone Conversion\n",
    "                if df.index.tz is None:\n",
    "                    if assume_utc:\n",
    "                        # UTC -> CET -> Naive\n",
    "                        df.index = df.index.tz_localize('UTC').tz_convert(TARGET_TZ).tz_localize(None)\n",
    "                else:\n",
    "                    # Aware -> CET -> Naive\n",
    "                    df.index = df.index.tz_convert(TARGET_TZ).tz_localize(None)\n",
    "\n",
    "                # 5. Sort by Time\n",
    "                df = df.sort_index()\n",
    "\n",
    "                # 6. Add Metadata\n",
    "                df['Quarter_ID'] = quarter_id\n",
    "                df['Is_In_Sample'] = quarter_id in IN_SAMPLE_QUARTERS\n",
    "\n",
    "                data_store[group_id][quarter_id] = df\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {filename}: {e}\")\n",
    "\n",
    "    return data_store"
   ],
   "id": "5a174da3e8cdc694",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-14T14:26:46.008106Z",
     "start_time": "2026-01-14T14:26:46.004269Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ---------------------------------------------------------\n",
    "# 3. Helper Function to Combine Data\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "def combine_quarters(data_store, group_id, only_in_sample=True):\n",
    "    if group_id not in data_store:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    quarters_dict = data_store[group_id]\n",
    "    df_list = []\n",
    "\n",
    "    for q_id, df in quarters_dict.items():\n",
    "        if only_in_sample and not df['Is_In_Sample'].iloc[0]:\n",
    "            continue\n",
    "        df_list.append(df)\n",
    "\n",
    "    if df_list:\n",
    "        combined_df = pd.concat(df_list)\n",
    "        combined_df = combined_df.sort_index()\n",
    "        return combined_df\n",
    "    else:\n",
    "        return pd.DataFrame()"
   ],
   "id": "43acc6808ca87e88",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-14T14:26:46.019760Z",
     "start_time": "2026-01-14T14:26:46.014266Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ---------------------------------------------------------\n",
    "# 4. Execution Example\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "def apply_common_assumptions(df, group_type):\n",
    "    \"\"\"\n",
    "    Applies trading rules, session times, and exclusion zones.\n",
    "    \"\"\"\n",
    "    if df.empty: return df\n",
    "\n",
    "    # Initialize flags\n",
    "    df['can_trade'] = False\n",
    "    df['force_exit'] = False\n",
    "\n",
    "    t = df.index.time\n",
    "\n",
    "    # =========================================================\n",
    "    # GROUP 1: SP, NQ (1-min)\n",
    "    # =========================================================\n",
    "    if group_type == \"group1\":\n",
    "        # 1. Calculation Exclusion (Set prices to NaN)\n",
    "        mask_morning_nan = (t >= time(9, 31)) & (t <= time(9, 40))\n",
    "        mask_evening_nan = (t >= time(15, 51)) & (t <= time(16, 0))\n",
    "\n",
    "        cols_to_nan = [c for c in df.columns if c not in ['Quarter_ID', 'Is_In_Sample', 'can_trade', 'force_exit']]\n",
    "        df.loc[mask_morning_nan | mask_evening_nan, cols_to_nan] = np.nan\n",
    "\n",
    "        # 2. Trading Window (09:56 - 15:40)\n",
    "        trade_start = time(9, 56)\n",
    "        exit_time = time(15, 40)\n",
    "\n",
    "        mask_trade = (t >= trade_start) & (t <= exit_time)\n",
    "        df.loc[mask_trade, 'can_trade'] = True\n",
    "\n",
    "        # 3. Force Exit\n",
    "        df.loc[t == exit_time, 'force_exit'] = True\n",
    "\n",
    "        # Ensure cannot trade after exit\n",
    "        df.loc[t > exit_time, 'can_trade'] = False\n",
    "\n",
    "    # =========================================================\n",
    "    # GROUP 2: FX/Metals (5-min)\n",
    "    # =========================================================\n",
    "    elif group_type == \"group2\":\n",
    "        # 1. Row Removal (Gap Creation)\n",
    "        keep_mask = (t < time(16, 50)) | (t >= time(18, 10))\n",
    "        df = df.loc[keep_mask].copy()\n",
    "\n",
    "        # Re-index time after removal\n",
    "        t = df.index.time\n",
    "\n",
    "        # 2. Trading Window\n",
    "        df['can_trade'] = True\n",
    "\n",
    "        # 3. Force Exit (16:45)\n",
    "        df.loc[t == time(16, 45), 'force_exit'] = True\n",
    "\n",
    "        # 4. Restart Logic (handled by return calculation gap check)\n",
    "\n",
    "    return df"
   ],
   "id": "d0cc6c44d740b4b6",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-14T14:26:46.034313Z",
     "start_time": "2026-01-14T14:26:46.028851Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ---------------------------------------------------------\n",
    "# 5. Return & Volatility Function (Session-based Grouping)\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "def add_return_and_volatility(df, assets, interval_min, vol_window=20):\n",
    "    \"\"\"\n",
    "    Adds Log Returns and Annualized Volatility.\n",
    "    Uses 'Session ID' based on time gaps to robustly handle breaks and day boundaries.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    if df.empty: return df\n",
    "\n",
    "    # 1. Dynamic Annualization Factor\n",
    "    daily_counts = df.groupby(df.index.date).size()\n",
    "    bars_per_day_mode = daily_counts.mode()[0] if not daily_counts.empty else (24*60/interval_min)\n",
    "    annual_factor = np.sqrt(bars_per_day_mode * 252)\n",
    "\n",
    "    print(f\"Stats for {assets}:\")\n",
    "    print(f\"  - Freq: {interval_min} min\")\n",
    "    print(f\"  - Bars/Day (Mode): {bars_per_day_mode}\")\n",
    "    print(f\"  - Annual Factor: {annual_factor:.2f}\")\n",
    "\n",
    "    # 2. Create Session ID based on Time Gaps\n",
    "    # This detects overnight gaps AND intraday breaks (Group 2 17:00-18:00)\n",
    "    # Threshold: slightly larger than the interval (e.g., interval + 30s tolerance)\n",
    "    time_diff = df.index.to_series().diff()\n",
    "    max_gap = pd.Timedelta(minutes=interval_min) + pd.Timedelta(seconds=30)\n",
    "\n",
    "    # A new session starts if time difference > max_gap\n",
    "    # First row is always a new session (fillna)\n",
    "    is_new_session = (time_diff > max_gap).fillna(True)\n",
    "    session_id = is_new_session.cumsum()\n",
    "\n",
    "    # 3. Calculate Returns & Volatility\n",
    "    for asset in assets:\n",
    "        if asset not in df.columns:\n",
    "            continue\n",
    "\n",
    "        # --- A. Log Return Calculation (Session-based) ---\n",
    "        # Group by session_id prevents calculating returns across gaps/breaks/nights.\n",
    "        df[f'{asset}_rtn'] = df.groupby(session_id)[asset].apply(\n",
    "            lambda x: np.log(x / x.shift(1))\n",
    "        )\n",
    "\n",
    "        # --- B. Volatility Calculation ---\n",
    "        # Calculate volatility within each continuous session\n",
    "        df[f'{asset}_vol'] = df.groupby(session_id)[f'{asset}_rtn'].transform(\n",
    "            lambda x: x.rolling(window=vol_window).std()\n",
    "        ) * annual_factor\n",
    "\n",
    "    return df"
   ],
   "id": "10555b626ae53079",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-14T14:29:10.128456Z",
     "start_time": "2026-01-14T14:29:10.002523Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ---------------------------------------------------------\n",
    "# 5. Return & Volatility Function (Session-based Grouping)\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "def add_return_and_volatility(df, assets, interval_min, vol_window=20):\n",
    "    \"\"\"\n",
    "    Adds Log Returns and Annualized Volatility.\n",
    "    Uses 'Session ID' based on time gaps.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    if df.empty: return df\n",
    "\n",
    "    # 1. Dynamic Annualization Factor\n",
    "    daily_counts = df.groupby(df.index.date).size()\n",
    "    bars_per_day_mode = daily_counts.mode()[0] if not daily_counts.empty else (24*60/interval_min)\n",
    "    annual_factor = np.sqrt(bars_per_day_mode * 252)\n",
    "\n",
    "    print(f\"  [Stats] Freq: {interval_min}m | Bars/Day: {bars_per_day_mode} | AnnFactor: {annual_factor:.2f}\")\n",
    "\n",
    "    # 2. Create Session ID based on Time Gaps\n",
    "    time_diff = df.index.to_series().diff()\n",
    "    max_gap = pd.Timedelta(minutes=interval_min) + pd.Timedelta(seconds=30)\n",
    "    is_new_session = (time_diff > max_gap).fillna(True)\n",
    "    session_id = is_new_session.cumsum()\n",
    "\n",
    "    # 3. Calculate Returns & Volatility\n",
    "    for asset in assets:\n",
    "        if asset not in df.columns:\n",
    "            continue\n",
    "\n",
    "        # Log Return (Session-based)\n",
    "        df[f'{asset}_rtn'] = df.groupby(session_id)[asset].transform(\n",
    "            lambda x: np.log(x / x.shift(1))\n",
    "        )\n",
    "\n",
    "        # Volatility\n",
    "        df[f'{asset}_vol'] = df.groupby(session_id)[f'{asset}_rtn'].transform(\n",
    "            lambda x: x.rolling(window=vol_window).std()\n",
    "        ) * annual_factor\n",
    "\n",
    "    return df"
   ],
   "id": "201b76a9f73e7c81",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-14T14:29:12.580472Z",
     "start_time": "2026-01-14T14:29:10.846750Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ---------------------------------------------------------\n",
    "# 6. Main Execution Block\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "def main():\n",
    "    print(\"=== Starting Data Processing pipeline ===\")\n",
    "\n",
    "    # 1. Load Data\n",
    "    # assume_utc=True: Naive timestamps are treated as UTC, then converted to CET\n",
    "    raw_data = load_project_data(DATA_DIR, assume_utc=True)\n",
    "\n",
    "    if not raw_data[\"group1\"] and not raw_data[\"group2\"]:\n",
    "        print(\"No data loaded. Exiting.\")\n",
    "        return\n",
    "\n",
    "    # -----------------------------------------------------\n",
    "    # Process Group 1 (SP, NQ)\n",
    "    # -----------------------------------------------------\n",
    "    print(\"\\n--- Processing Group 1 (SP, NQ) ---\")\n",
    "    df_g1 = combine_quarters(raw_data, \"group1\", only_in_sample=True)\n",
    "\n",
    "    if not df_g1.empty:\n",
    "        # Step A: Apply Rules\n",
    "        df_g1_clean = apply_common_assumptions(df_g1.copy(), \"group1\")\n",
    "\n",
    "        # Step B: Add Metrics (Overwriting df_g1_clean to include metrics)\n",
    "        df_g1_clean = add_return_and_volatility(\n",
    "            df_g1_clean,\n",
    "            assets=['SP', 'NQ'],\n",
    "            interval_min=1\n",
    "        )\n",
    "        print(f\"Group 1 processed shape: {df_g1_clean.shape}\")\n",
    "\n",
    "    # -----------------------------------------------------\n",
    "    # Process Group 2 (FX/Metals)\n",
    "    # -----------------------------------------------------\n",
    "    print(\"\\n--- Processing Group 2 (FX, Metals) ---\")\n",
    "    df_g2 = combine_quarters(raw_data, \"group2\", only_in_sample=True)\n",
    "\n",
    "    if not df_g2.empty:\n",
    "        # Step A: Apply Rules\n",
    "        df_g2_clean = apply_common_assumptions(df_g2.copy(), \"group2\")\n",
    "\n",
    "        # Identify assets\n",
    "        g2_assets = ['AUD', 'CAD', 'XAU', 'XAG']\n",
    "        existing_assets = [c for c in g2_assets if c in df_g2_clean.columns]\n",
    "\n",
    "        # Step B: Add Metrics (Overwriting df_g2_clean to include metrics)\n",
    "        df_g2_clean = add_return_and_volatility(\n",
    "            df_g2_clean,\n",
    "            assets=existing_assets,\n",
    "            interval_min=5\n",
    "        )\n",
    "        print(f\"Group 2 processed shape: {df_g2_clean.shape}\")\n",
    "\n",
    "    # -----------------------------------------------------\n",
    "    # Save Output (User Requested Format)\n",
    "    # -----------------------------------------------------\n",
    "    print(\"\\n--- Saving Data ---\")\n",
    "\n",
    "    # Save processed dataframes to pickle files for the next step (EDA)\n",
    "    # This preserves column types and indices perfectly.\n",
    "\n",
    "    if 'df_g1_clean' in locals() and not df_g1_clean.empty:\n",
    "        df_g1_clean.to_pickle('df_g1_processed.pkl')\n",
    "        print(\"Saved: df_g1_processed.pkl\")\n",
    "\n",
    "    if 'df_g2_clean' in locals() and not df_g2_clean.empty:\n",
    "        df_g2_clean.to_pickle('df_g2_processed.pkl')\n",
    "        print(\"Saved: df_g2_processed.pkl\")\n",
    "\n",
    "    print(\"Data saved successfully to .pkl files.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "id": "c6f18cd3438e6bb3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Starting Data Processing pipeline ===\n",
      "Found 14 files. Loading...\n",
      "\n",
      "--- Processing Group 1 (SP, NQ) ---\n",
      "  [Stats] Freq: 1m | Bars/Day: 390 | AnnFactor: 313.50\n",
      "Group 1 processed shape: (175812, 10)\n",
      "\n",
      "--- Processing Group 2 (FX, Metals) ---\n",
      "  [Stats] Freq: 5m | Bars/Day: 272 | AnnFactor: 261.81\n",
      "Group 2 processed shape: (122630, 16)\n",
      "\n",
      "--- Saving Data ---\n",
      "Saved: df_g1_processed.pkl\n",
      "Saved: df_g2_processed.pkl\n",
      "Data saved successfully to .pkl files.\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "a5bbc64fea9d595a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
